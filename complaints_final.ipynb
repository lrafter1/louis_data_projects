{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2cda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the data from the CSV file\n",
    "df = pd.read_csv(\"complaints.csv\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"issue\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Vectorize the training and testing data using TF-IDF\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model on the TF-IDF data\n",
    "lr_tfidf = LogisticRegression()\n",
    "lr_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_lr_tfidf = lr_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Print the accuracy and classification report for the logistic regression model\n",
    "print(\"TF-IDF Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr_tfidf))\n",
    "print(classification_report(y_test, y_pred_lr_tfidf))\n",
    "\n",
    "# Create a Word2Vec model\n",
    "w2v_model = Word2Vec(size=100, min_count=1)\n",
    "w2v_model.build_vocab(X_train.str.split())\n",
    "w2v_model.train(X_train.str.split(), total_examples=w2v_model.corpus_count, epochs=w2v_model.epochs)\n",
    "\n",
    "# Vectorize the training and testing data using Word2Vec\n",
    "X_train_w2v = pd.DataFrame([pd.Series(x).apply(lambda x: w2v_model.wv[x]) for x in X_train.str.split()])\n",
    "X_test_w2v = pd.DataFrame([pd.Series(x).apply(lambda x: w2v_model.wv[x]) for x in X_test.str.split()])\n",
    "\n",
    "# Train a random forest model on the Word2Vec data\n",
    "rf_w2v = RandomForestClassifier()\n",
    "rf_w2v.fit(X_train_w2v, y_train)\n",
    "y_pred_rf_w2v = rf_w2v.predict(X_test_w2v)\n",
    "\n",
    "# Print the accuracy and classification report for the random forest model\n",
    "print(\"Word2Vec Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf_w2v))\n",
    "print(classification_report(y_test, y_pred_rf_w2v))\n",
    "\n",
    "# Create a DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the training and testing data using DistilBERT\n",
    "X_train_bert = tokenizer(X_train.tolist(), truncation=True, padding=True)\n",
    "X_test_bert = tokenizer(X_test.tolist(), truncation=True, padding=True)\n",
    "\n",
    "# Convert the tokenized data to TensorFlow tensors\n",
    "X_train_bert = {key: tf.convert_to_tensor(val) for key, val in X_train_bert.items()}\n",
    "X_test_bert = {key: tf.convert_to_tensor(val) for key, val in X_test_bert.items()}\n",
    "\n",
    "# Pass the tokenized data through the DistilBERT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa378bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288122be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the CSV file\n",
    "df = pd.read_csv(\"complaints.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"issue\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccbd825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be9df5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the training and testing data using TF-IDF\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46cefe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression model on the TF-IDF data\n",
    "lr_tfidf = LogisticRegression()\n",
    "lr_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_lr_tfidf = lr_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the accuracy and classification report for the logistic regression model\n",
    "print(\"TF-IDF Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr_tfidf))\n",
    "print(classification_report(y_test, y_pred_lr_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac1176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Word2Vec model\n",
    "w2v_model = Word2Vec(size=100, min_count=1)\n",
    "w2v_model.build_vocab(X_train.str.split())\n",
    "w2v_model.train(X_train.str.split(), total_examples=w2v_model.corpus_count, epochs=w2v_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d825d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the training and testing data using Word2Vec\n",
    "X_train_w2v = pd.DataFrame([pd.Series(x).apply(lambda x: w2v_model.wv[x]) for x in X_train.str.split()])\n",
    "X_test_w2v = pd.DataFrame([pd.Series(x).apply(lambda x: w2v_model.wv[x]) for x in X_test.str.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f0da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a random forest model on the Word2Vec data\n",
    "rf_w2v = RandomForestClassifier()\n",
    "rf_w2v.fit(X_train_w2v, y_train)\n",
    "y_pred_rf_w2v = rf_w2v.predict(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28842354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the accuracy and classification report for the random forest model\n",
    "print(\"Word2Vec Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf_w2v))\n",
    "print(classification_report(y_test, y_pred_rf_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb16ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training and testing data using DistilBERT\n",
    "X_train_bert = tokenizer(X_train.tolist(), truncation=True, padding=True)\n",
    "X_test_bert = tokenizer(X_test.tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5797a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tokenized data to TensorFlow tensors\n",
    "X_train_bert = {key: tf.convert_to_tensor(val) for key, val in X_train_bert.items()}\n",
    "X_test_bert = {key: tf.convert_to_tensor(val) for key, val in X_test_bert.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8878137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the tokenized data through the DistilBERT model\n",
    "X_train_bert = model(X_train_bert)[0][:, 0, :].numpy()\n",
    "X_test_bert = model(X_test_bert)[0][:, 0, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19cb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a linear SVM model on the DistilBERT data\n",
    "svm_bert = LinearSVC()\n",
    "svm_bert.fit(X_train_bert, y_train)\n",
    "y_pred_svm_bert = svm_bert.predict(X_test_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb587f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the accuracy and classification report for the linear SVM model\n",
    "print(\"DistilBERT Linear SVM Accuracy:\", accuracy_score(y_test, y_pred_svm_bert))\n",
    "print(classification_report(y_test, y_pred_svm_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e5fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the training and testing data using a custom embedding layer\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=len(tfidf_vectorizer.vocabulary_), output_dim=100, input_length=max([len(x.split()) for x in X_train]), weights=[w2v_model.wv.vectors])\n",
    "model = tf.keras.models.Sequential([\n",
    "embedding_layer,\n",
    "tf.keras.layers.Flatten(),\n",
    "tf.keras.layers.Dense(units=128, activation=\"relu\"),\n",
    "tf.keras.layers.Dense(units=64, activation=\"relu\"),\n",
    "tf.keras.layers.Dense(units=len(df[\"issue\"].unique()), activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "y_pred_custom = model.predict(X_test)\n",
    "y_pred_custom = [df[\"issue\"].unique()[i] for i in y_pred_custom.argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf939b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the accuracy and classification report for the custom embedding layer model\n",
    "print(\"Custom Embedding Layer Accuracy:\", accuracy_score(y_test, y_pred_custom))\n",
    "print(classification_report(y_test, y_pred_custom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f913b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code creates four different machine learning models for predicting the issue based on consumer \n",
    "#complaint narratives using different vectorization methods. \n",
    "\n",
    "#First, the code uses a TF-IDF vectorizer to convert the training and testing data into TF-IDF vectors. \n",
    "#It then trains a logistic regression model on the TF-IDF data and prints the accuracy and classification report \n",
    "#for the model.\n",
    "\n",
    "#Next, the code uses Word2Vec to create embeddings for the training and testing data. It then trains a random \n",
    "#forest model on the Word2Vec data and prints the accuracy and classification report for the model.\n",
    "\n",
    "#The code also uses DistilBERT to tokenize the training and testing data and passes the tokenized data through \n",
    "#the model to obtain embeddings. It then trains a linear SVM model on the DistilBERT embeddings and prints \n",
    "#the accuracy and classification report for the model.\n",
    "\n",
    "#Finally, the code uses a custom embedding layer in a TensorFlow model to convert the text data into embeddings. \n",
    "#It then trains the model on the training data and prints the accuracy and classification report for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50f404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f83084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f151da09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
